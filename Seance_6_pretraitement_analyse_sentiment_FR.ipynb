{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Mégadonnées et techniques avancées démystifiées**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*Nouvelles méthodes d’analyse et leur implication quant à la gestion des mégadonnées en SSH (partie 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "Cet atelier s’inscrit dans le cadre de la formation [Mégadonnées et techniques avancées démystifiées](https://www.4point0.ca/2022/08/22/formation-megadonnees-demystifiees/) (séance 6).\n",
        "\n",
        "Les sciences humaines et sociales sont souvent confrontées à l’analyse de données non structurées, comme le texte. Après avoir préparé les données, plusieurs techniques d’analyse venant de l’apprentissage automatique peuvent être utilisées. Pendant cet atelier, les participants seront initiés au prétraitement des données textuelles et aux méthodes supervisées et non supervisées à des buts d’analyse avec Python.\n",
        "\n",
        "Note : Cet atelier se poursuit lors d’une 2e séance le **10 novembre 2022**. Les deux séances ne peuvent pas être considerées exaustives du domaine.\n",
        "\n",
        "Structure de l'atelier :\n",
        "1. Partie 1 : Présentation en mode plénière de la [Section 1](#Section_1) (40 minutes)\n",
        "2. Partie 2 : Travail individuel sur la [Section 2](#Section_2) (10 minutes)\n",
        "3. Partie 3 : Travail en équipe sur la [Section 2](#Section_2) (30 minutes)\n",
        "4. Partie 4 : Conclusion en mode plénière (10 minutes)\n",
        "\n",
        "### Auteurs: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "Département de Mathématiques et de génie industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>0. Préparation de l'environnement </font>"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jddhTL7EFrN_"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Data_techniques_demystified_webinars/\n",
        "!git clone https://github.com/4point0-ChairInnovation-Polymtl/Data_techniques_demystified_webinars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJLRFWa6FrOA"
      },
      "outputs": [],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'E3A440'>**0.1 Les bases de Python pour réaliser cet atelier**</font>\n",
        "\n",
        "Nous présentons deux concepts de bases qui permettront aux personnes qui n'ont jamais écrit une ligne de code de s'orienter pendant cet atelier."
      ],
      "metadata": {
        "id": "1_J-haWRfr9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'E3A440'>Les variables</font>\n",
        "\n",
        "Les [variables](https://www.w3schools.com/python/python_variables.asp) sont des objets qui contiennent de l'information dans des formats très variés. Pour créer une variable, nous utilisons la syntaxe suivante: `nom_variable = contenu_de_la_variable`. Voici quelques exemples: "
      ],
      "metadata": {
        "id": "h9OcrUM3ghbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with characters\n",
        "variable_avec_caracteres = 'contenu de la variable avec caractères'"
      ],
      "metadata": {
        "id": "2NOP4itmggHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(variable_avec_caracteres)"
      ],
      "metadata": {
        "id": "ZY32KyFIl01S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with numbers\n",
        "variable_avec_nombre = 5\n",
        "print(variable_avec_nombre)"
      ],
      "metadata": {
        "id": "JYXwYERFjhIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with list of numbers\n",
        "variable_avec_liste_de_nombres = [5, 6, 7, 8, 9]\n",
        "print(variable_avec_liste_de_nombres)"
      ],
      "metadata": {
        "id": "Lqo9w4GtjmzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with tablar data\n",
        "variable_avec_donnees_tabulaires = pd.DataFrame([[1,2,3],[4,5,6]], columns = ['Col_1','Col_2','Col_3'], index = ['Row_1','Row_2'])"
      ],
      "metadata": {
        "id": "AGOkQeezj6Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(variable_avec_donnees_tabulaires)"
      ],
      "metadata": {
        "id": "DlOlvpICmPdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'E3A440'>Les fonctions</font>\n",
        "\n",
        "Une [fonction](https://www.w3schools.com/python/python_functions.asp) permet d'accomplir des calculs à partir d'une ou plusieurs variables, des données, etc. Les fonctions ont généralement la syntaxe suivante:  `resultat_de_la_function = fonction(argument_1 = contenu_argument_1, argument_2 = contenu_argument_2, etc.)`. Voici quelques exemples: "
      ],
      "metadata": {
        "id": "ovKqRMXtrfNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultat_somme = sum(variable_avec_liste_de_nombres)\n",
        "print(resultat_somme)"
      ],
      "metadata": {
        "id": "y08LpuNUrbVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame( data = [[1,2,3],[4,5,6]], columns = ['Col_1','Col_2','Col_3'], index = ['Row_1','Row_2'] )"
      ],
      "metadata": {
        "id": "qitg26AnuTmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "<a name='Section_1'></a>\n",
        "# <font color = 'E3A440'>1. *Préparation des données textuelles*</font>\n",
        "\n",
        "L'analyse de données textuelles implique la transformation d'un texte en un objet mathématique qui peut être utilisé par des algorithmes et des modèles statistiques. Cette étape est importante car elle permet de **structurer** des données non structurées, comme le texte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**1.1 Étapes fondamentales du prétraitement**</font>\n",
        "\n",
        "Prenons la phrase suivante pour illustrer les étapes qui nous permettront de la transformer en information structurée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour l'instant, `sentence` est simplement une chaine de caractères. On peut compter le nombre de caractères qui composent cette variable."
      ],
      "metadata": {
        "id": "eqqNBHnM57ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFqEU4iFrOC"
      },
      "outputs": [],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Connaitre le nombre de caractères dans un texte, n'est peut être pas suffisant pour analyser son contenu :-).\n",
        "\n",
        "Nous allons voir dans la suite différents outils d'analyse."
      ],
      "metadata": {
        "id": "59CuGGAL6KSN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*a. Tokenisation*</font>\n",
        "\n",
        "Tout d'abord, il peut être utile de découper la chaine de caractères initiale en unités linguistiques élémentaires et dotées de sens, généralement appelés \"mots\".\n",
        "\n",
        "Dans le module `nltk`, il existe une fonction (`word_tokenize()`) qui permet de réaliser cette opération."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tzF3fiUWFrOE"
      },
      "outputs": [],
      "source": [
        "# La function word_tokenize() prend la phrase comme argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Analyse morphosyntaxique*</font>\n",
        "\n",
        "Après avoir identifé tous les mots, il est possible d'analyser leur rôle morphosyntaxique, à des fins d'analyse et/ou filtrage. "
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La function word_tokenize() prend la liste de mots comme argument.\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la liste de possibles POS tags:"
      ],
      "metadata": {
        "id": "6cECT-Gp2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "kUDRUVYe2kLi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*c. Retirer la ponctuation*</font>\n",
        "\n",
        "Une autre opération consiste à retirer la ponctuation. Ce type de filtrage réduit le nombre de signes graphiques qui participent le moins à la construction de la sémantique de la phrase. \n",
        "Dans certains contextes, comme en stylométrie, ce processus peut être exécuté avec des techniques plus sophistiquées. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0P7SkM5RFrOF"
      },
      "outputs": [],
      "source": [
        "# La ligne de code suivante itère sur chaque \"mot\" et retient ceux qui sont composés uniquement de caractères alphanumériques.\n",
        "words_pos1 = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos1)\n",
        "len(words_pos1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Il serait possible aussi d'utiliser le résultat de l'analyse morphosyntaxique pour éliminer la ponctuaction\n",
        "words_pos2 = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "print(words_pos2)\n",
        "len(words_pos2)"
      ],
      "metadata": {
        "id": "S30VdMLP_6lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remarquez la différence: les \"mots\" `o'clock` et `n't` sont absents de la première liste, mais présents dans la seconde."
      ],
      "metadata": {
        "id": "QpCrw1VMAcbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_pos = words_pos2"
      ],
      "metadata": {
        "id": "hVEip9IyBOcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*d. Convertir chaque caractère en minuscule*</font>\n",
        "\n",
        "Cette étape constitue une première opération de normalisation des mots et de leur réduction à une forme graphique unique. Ce genre d'étape permet de regrouper chaque occurence d'un mot sous une seule forme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8tdwmniFrOG"
      },
      "outputs": [],
      "source": [
        "# La ligne de code suivante itère sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est encore une opération a mener avec prudence : USA, UNESCO, Dr., les noms propres, etc. doivent conserver leurs majuscules."
      ],
      "metadata": {
        "id": "QpCrw1VMAcbJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*e. Retirer les stopwords (mots vides)*</font>\n",
        "\n",
        "Une autre opération de filtrage consiste à éliminer les mots fonctionnels. Cette liste de mots contient tout les connecteurs de phrases, comme \"et\", \"mais\", \"toutefois\" et des mots avec une faible valeur sémantique, comme les verbes modaux. \n",
        "Comme d'autres opérations de filtrage, l'enjeux est celui de nettoyer le plus possible le vocabulaire et de reduire toutes les occurrences d'un mot sous une forme graphique unique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icU72m20FrOH"
      },
      "outputs": [],
      "source": [
        "# Nous importons la liste de stopword en anglais\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ws2STtGKFrOH"
      },
      "outputs": [],
      "source": [
        "# La ligne de code suivante itère sur chaque mot et garde ceux qui ne sont pas dans la liste de stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*f. Rammener les mots à leur racine*</font> \n",
        "\n",
        "En suivant le même objectif, nous retirons le suffixe morphologique des mots, ce qui augmente le niveau de réduction de chaque occurrence d'un mot à une unique forme graphique.\n",
        "\n",
        "Il existe deux méthodes fondamentales: la racinisaiton et la lemmatisation.\n",
        "La première réduit les occurences à une racine qui est inférée au moyen de plusieurs techniques, l'autre est la réduction de l'occurrence à son lemme. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_mhuA7zFrOI"
      },
      "outputs": [],
      "source": [
        "# Racinisation: technique Porter\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnsV3pSZFrOI"
      },
      "outputs": [],
      "source": [
        "# Racinisation: technique Lancaster\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN7uAtUgFrOJ"
      },
      "outputs": [],
      "source": [
        "# Lemmatisaiton: utilisant le thesaurus wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*g. Filtrage selon le rôle morphosyntaxique*</font>\n",
        "\n",
        "Le filtrage des unités lexicales peut s'étendre jusqu'à l'élimination d'unités qui ne font pas partie d'une liste de rôles morphosyntaxiques prédéfinie. Par exemple, nous pouvons retirer tous les mots qui ne sont pas de *noms* ou des *adjectifs*."
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retenir seulement les noms et les adjectifs\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "## <font color = 'E3A440'>**1.2 Traitement d'un corpus**</font>\n",
        "\n",
        "Le prétraitement d'un corpus de textes peut nécessiter la mise en place de plusieurs étapes. La première et la plus importante est le découpage du corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*1. Découpage du texte*</font>\n",
        "\n",
        "Tout dépendant de l'objectif de l'analyse, le texte peut être découpé en plusieurs fragments, chacun desquels peut être un document, un paragraphe, une concordance, un groupe de phrases, une phrase simple, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZqenN7Rcc2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOzWtcugFrOK"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le bloc de code suivant, nous faisons un découpage par phrase."
      ],
      "metadata": {
        "id": "sy8Qow43dHKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lihe1FNjFrOK"
      },
      "outputs": [],
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*2. Annotation et nettoyage*</font>\n",
        "\n",
        "Les opérations précédentes d'annotation morphosyntaxique et de filtrage seront appliquées à chaque fragment du corpus qui a été créé.\n"
      ],
      "metadata": {
        "id": "gHi0hAxZdKx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*a. Création d'une fonction*</font>\n",
        "\n",
        "Dans le code suivant, une fonction est créée pour englober toutes les opérations nécessaires pour l'annotation et le nettoyage."
      ],
      "metadata": {
        "id": "waMXvFxhgH0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# To run this function proprlely, you need to import modules needed\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Please, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Application du nettoyage*</font>\n",
        "\n",
        "Maintenant, nous pouvons apliquer cette fonction à chaque fragment de texte."
      ],
      "metadata": {
        "id": "kWkIfcMwpqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "id": "vkpxyQfl2Wna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*c. Fréquence des mots*</font>\n",
        "\n",
        "Quelle est la fréquence des mots de notre corpus? Pour répondre, nous créons une liste de mots en retirant l'annotation morphosyntaxique."
      ],
      "metadata": {
        "id": "TBNxooILr8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs_in_text = nltk.FreqDist([w for sent in cleaned_sentences for w, pos in sent ])\n",
        "freqs_in_text"
      ],
      "metadata": {
        "id": "NFPhg9Sm6Lbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorisation*</font>\n",
        "\n",
        "Généralement, pour utiliser le texte dans un contexte d'analyse de données ou d'apprentissage automatique, ce texte doit être transformé dans un objet mathématique approprié. \n",
        "Le modèle le plus simple et diffusé est le \"sac de mots\" (\"bags-of-words\"), dans lequel chaque texte (ou chaque fragment de texte) est défini dans un vecteur, par un certain nombre d'unités lexicales qui le caractérisent. Ce modèle appartient à la famille de modèles de la sémantique vectorielle et il a la forme suivante:\n",
        "\n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "x_{1,1} & x_{1,2} & \\ldots & x_{1,w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n,1} & x_{1,2} & \\ldots & x_{n,w} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "Dans cette matrice, la valeur $x_{i,j}$ représente le \"poids\" du mot $j$ dans le texte $i$. Ce poids peut prendre différentes valeurs selon ce que l'on cherche à faire. Ainsi :\n",
        "\n",
        "- $x_{i,j}$ peut représenter la présence du mot \"j\" dans le texte $i$,\n",
        "- $x_{i,j}$ peut mesurer le nombre d'occurences du mot $j$ dans le texte $i$,\n",
        "- $x_{i,j}$ peut représenter l'**importance** du mot $j$ dans le texte $i$, dans ce cas on utilisera par exemple la métrique tf-idf :\n",
        " $$\\text{tf-idf}_{i,j}=\\text{tf}_{i,j}.log\\left(\\frac{n}{n_i}\\right)$$\n",
        " - $\\text{tf}_{i,j}$ est la fréquence du terme $i$ dans le document $j$,\n",
        " - $n$ nombre total de documents dans l’ensemble de textes à étudier,\n",
        " - $n_i$ nombre de documents dans l’ensemble de textes qui contiennent le terme $i$.\n"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilisation du \"vectorizer\" avec une liste de listes de mots (et non une liste de tuples de mots-pos)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Liste de liste de mots:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "id": "gy8_VTM67Lax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application du vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "id": "v8ydVdmC5c76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Les valeurs des poids Tf-Idf sont stockés dans la variable `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "pRPjGQVokg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "print(pd.DataFrame(tfidf_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GsktR7_Znke",
        "outputId": "3133a1e3-9eee-4cd2-bacb-250c08e9b7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     arthur       dog      feel      felt  following      good     great  \\\n",
            "0  0.137750  0.000000  0.181125  0.000000   0.000000  0.181125  0.181125   \n",
            "1  0.215994  0.000000  0.000000  0.284006   0.284006  0.000000  0.000000   \n",
            "2  0.000000  0.333333  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
            "\n",
            "    morning       run    street  thursday  \n",
            "0  0.137750  0.000000  0.000000  0.181125  \n",
            "1  0.215994  0.000000  0.000000  0.000000  \n",
            "2  0.000000  0.333333  0.333333  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "<a name=\"Section_2\"></a>\n",
        "# <font color = 'E3A440'> 2. *Exercise : Analyse de sentiments sur Twitter* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'exercice qui est proposé dans cette section est basé sur une simple chaîne de traitement pour l'**analyse de sentiments** sur des données de Twitter et sur l'**analyse de spécificités lexicales**. \n",
        "\n",
        "Le corpus utilisé a été collecté en 2020 par *trackmyhashtag.com* et contient 150 000 tweets pour les 50 profils les plus suivis de Twitter. Les données sont en format tabulaire dans un fichier CSV. Pour des raisons pédagogiques, cet exercice prévoit l'utilisation d'un échantillon aléatoire de 5 000 tweets.\n",
        "\n",
        "Dans un premier temps, les données textuelles de 5 000 tweets seront analysées par un module d'analyse de sentiments du module `nltk`. Ensuite, le texte sera prétraité et certaines analyses lexicales seront executées.\n",
        "\n",
        "Pendant l'exercice, le participant sera invité à remplir les parties manquantes du code qui sont indiquées avec `...` (trois points)."
      ],
      "metadata": {
        "id": "c8XIxHR-n0Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.1 Présentation de l'exercice </font>"
      ],
      "metadata": {
        "id": "FAgZaK170IHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Importer les données </font>\n",
        "\n",
        "Le fichier avec les données est archivé dans un `.zip` et contient plus de 150 000 tweets. Pour de raisons pédagogiques, nous importons seulement 5 000 tweets de façon aléatoire. "
      ],
      "metadata": {
        "id": "IxtJNukjCXT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Data_techniques_demystified_webinars/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with zipfile.ZipFile(os.path.join(DATA_DIR,'4POINT0_Top_50_tweet_profiles.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "df = pd.read_pickle(os.path.join(DATA_DIR,'Top_50_tweet_profiles.pkl')).sample(5000, random_state = 5641).reset_index()"
      ],
      "metadata": {
        "id": "Q6snBPy8EfBX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici les noms de variables disponibles."
      ],
      "metadata": {
        "id": "hDTMmSBhFA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "ejgQAfv8FIrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici une observation (une ligne du tableau de données):"
      ],
      "metadata": {
        "id": "JIDKgUxtGhQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "54zPNygEJXhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Exécuter l'analyse de sentiments</font>\n",
        "\n",
        "L'objet `SentimentIntensityAnalyzer` est utilisé pour exécuter l'analyse de sentiments. L'objet doit être initialisé et, ensuite, la fonction `polarity_scores()` peut être appliquée à une chaîne de caractères."
      ],
      "metadata": {
        "id": "ym3Y6ITXGkkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pInoj9yrFrOZ"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici trois exemples d'analyse de sentiments. Le résulat de la fonction `polarity_scores()` retourne quatre valeurs: \n",
        "\n",
        " 1. `neg` : indique le dégré, dans une échelle de 0 à 1, de sentiment négatif du texte.\n",
        " 2. `neu` : indique le dégré, dans une échelle de 0 à 1, de sentiment neutre du texte.\n",
        " 3. `pos` : indique le dégré, dans une échelle de 0 à 1, de sentiment positif du texte.\n",
        " 4. `compound` : contient une valeur composée des trois métriques précédentes et va de -1 à 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "sggH1sSIIVib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAgx5YgFrOa"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"Wow, Montreal Canadiens is the greatest hockey team in the world!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(\"Ottawa is not bad city!\")"
      ],
      "metadata": {
        "id": "ifi6teSvITFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGJWGRWFrOa"
      },
      "outputs": [],
      "source": [
        "sia.polarity_scores(\"No, you cannot put pineapple on a pizza! This is disgusting!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le tweet sur lesquels nous appliquons l'analyse de sentiments:"
      ],
      "metadata": {
        "id": "3gfG4_A6I4WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "id": "5yBg-jU9QLl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans le prochain bloc de code, nous exécutons l'analyse de sentiments sur la colonne `Tweet Content`, et nous ajoutons les résulats obtenus au tableau des données, l'objet nommé `df`."
      ],
      "metadata": {
        "id": "aCfTrGKYNOcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exécution de l'analyse de sentiments sur tout le corpus\n",
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le résultat de l'analyse est enregistré sous forme de variables. Voici un exemple:"
      ],
      "metadata": {
        "id": "bpauZXt7Nwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "yfrp02peNwSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour rendre simple l'analyse, nous utiliserons seulement la métrique composée `compound` qui est automatiquement calculée par la fonction  `polarity_score()`."
      ],
      "metadata": {
        "id": "wsjs2wOmbs0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "id": "Xf_vn9Veebrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour utiliser la métrique `compound` dans un conteste d'**analyse de spécificité lexicale**, il est utile de constituer des catégories, soit de regrouper les tweets sous les catégories suivantes:\n",
        " 1. `negative` : qui regroupe les tweets contenant un sentiment négatif (`compound` de -1 à -0.1)  \n",
        " 2. `neu` : qui regroupe les tweets plustôt neutres (`compound` de -0.5 à 0.5)\n",
        " 3. `positive` : qui regroupe les tweets contenant un sentiment positif (`compound` plus de 0.5)"
      ],
      "metadata": {
        "id": "A6cLssRqb81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Déterminer les valeurs pour couper la métrique compound\n",
        "bins = [-1, -0.1, 0.5, 1]\n",
        "# 2 Déterminer les noms des categories. NOTEZ que les nombres de noms de catégories doivent être inférieurs aux valeurs de découpage.\n",
        "names = ['negative', 'neu', 'positive']\n",
        "# Exécuter le découpage avec la fonction 'cut' de pandas.\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la distribution des tweets par catégorie:"
      ],
      "metadata": {
        "id": "-cRjoTWDdJjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['compound_category'])"
      ],
      "metadata": {
        "id": "UL-ibgrb4u0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Annotation, nettoyage et vectorisation des tweets </font>\n",
        "\n",
        "Nous utilisons la fonction écrite précédemment pour nettoyer les unités lexicales de tweets. Pour ce premier test, nous conservons seulement les adjectifs.\n",
        "\n",
        "Cette opération prendra quelques secondes. "
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['ADJ'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dans l'étape de vectorisation, nous retenons les mots qui apparaissent dans au moins 5 documents (`min_df = 5`)."
      ],
      "metadata": {
        "id": "TA9YRrWiLzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation de l'objet\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "wkSkuQvUXsvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> d. Analyse des spécificités lexicales </font>\n",
        "\n",
        "L'analyse de spécificités lexicales permet de mettre en évidence les unités lexicales qui sont spécifiques à un groupe particulier de données. Dans notre cas, il est possible d'identifier les mots qui sont plus fortement associés avec des sentiments positif ou négatif. \n",
        "\n",
        "Pour se faire, nous utilisons une métrique très diffusée en lexicométrie, qui est la fonction de vraisemblance (log-likelihood Ratio). La métrique est basée sur cet [article](https://aclanthology.org/J93-1003.pdf). D'autres méthodes peuvent être utilisées, comme l'information mutuelle, le chi2 ou la ponderation tf-idf."
      ],
      "metadata": {
        "id": "RtSfYp1rfUxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetLexicalSpecificities(freq_term_DTM, logical_vector):\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "    import math\n",
        "    df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "    return df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour exécuter l'analyse de spécificité, il est nécessaire de créer un vecteur logique (avec des valeurs binaires) qui indique par `True` la classe pour laquelle nous voulons analyser la spécificité lexicale et par `False` le reste du corpus. "
      ],
      "metadata": {
        "id": "SIxwMCSahwh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'\n",
        "logical_vector"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "id": "ogaDNEUNZQU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exécuter la fonction avec la matrice des fréquences (`freq_term_DTM`) et le vecteur logique que nous avons créé plus haut."
      ],
      "metadata": {
        "id": "G3lrLVXfiMMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8iEQNaJvJ2fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del logical_vector, freq_term_DTM"
      ],
      "metadata": {
        "id": "F9vvmgX46O7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.2 Exercice </font>\n",
        "\n",
        "Pendant l'exercice, le participant sera invité à remplir les parties manquantes du code qui sont indiquées avec `...` (trois points).\n",
        "\n",
        "Plusieurs manipulations et différents résultats seront demandés. Chaque sous-exercice suit la chaîne de traitement suivante :\n",
        "\n",
        "1. Annotation et nettoyage des tweets : le participant devra ajuster quelques paramètres de la fonction pour choisir un filtrage spécifique.\n",
        "2. Vectorisation :  le participant devra ajuster quelques paramètres de la fonction pour choisir un filtrage spécifique.\n",
        "3. Création d'un vecteur logique pour définir le groupe cible et le groupe de référence. \n",
        "4. Application de la fonction `GetLexicalSpecificities()` pour obtenir les 50 mots les plus spécifiques au groupe cible.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8zRK37miNzH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Étudier l'impact du filtrage morphosyntaxique sur les spécificités lexicales </font>\n",
        "\n",
        "Au point 2.1, seulement les adjectifs ont été étudiés. Faites maintenant une étude sur les noms, adjectifs et verbes et ensuite sur d'autres combinaisons qui vous intéressent. \n",
        "Voici la liste des POS tag existants:"
      ],
      "metadata": {
        "id": "Niv5IUK8VJ8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "CJps3VlyWcoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insérez la bonne valeur pour l'argument `list_pos_to_keep` afin de pouvoir garder les noms, adjectifs et verbes, ou toute autre combination de POS tag de votre intérêt.\n",
        "Regardez les trois points `...` et remplissez les avec votre réponse. Vous devriez vous inspirer par le code qui a été présenté pendant l'atelier. Le copier-coller est admis! "
      ],
      "metadata": {
        "id": "b4bLCBgqWlzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "xtrWcUOmNxOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changez les paramètres `min_df` afin de ne pas dépasser **750 mots** de votre matrice <font color='E3A440'>**Document-Term matrix**</font>, qui est enregistrée dans l'objet `freq_term_DTM`.\n",
        "\n",
        "Notez bien que dans cette fonction le paramètre `ngram_range` est configuré pour avoir les unigrams et les bigrams (sa valeur est : `(1,2)`)."
      ],
      "metadata": {
        "id": "g6JapeP7W2xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "    \n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 2), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "EoTIyCR7daI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant le travail fait déjà au point **b.** de la section **2.1**, choisissez la catégorie de sentiment pour laquelle vous voulez étudier la spécificité lexicale ex.  `negative` or `positive`."
      ],
      "metadata": {
        "id": "PC98Vv87YCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == ..."
      ],
      "metadata": {
        "id": "B-qMBlPhPGC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant la fonction définie au point **d.** de la section **2.1**, ajoutez les arguments fondamentaux de la fonction, soit la matrice  <font color='E3A440'>**Document-Term matrix**</font> et le **vecteur logique** créé dans le bloc de code précédent."
      ],
      "metadata": {
        "id": "kUkik8fxYXe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function needs 2 arguemnts: 1st is the matrix, the 2nd is the logical vector\n",
        "GetLexicalSpecificities(..., ...)"
      ],
      "metadata": {
        "id": "CGFUrGPVPJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Étudier de nouvelles categories basées sur le nombre de Retweet </font>\n",
        "\n",
        "Sur Twitter, il est possible de retweetter un tweet existant. Le nombre de retweet peut être considéré comme un indicateur du suivi qu'un tweet a obtenu. \n",
        "\n",
        "Répondez à la question suivante:  quelles sont les spécificités lexicales des tweets qui ont eu un très grand suivi ? \n",
        "\n",
        "Pour répondre, vous devez manipuler les lignes de code en accomplissant les étapes apprises au long de cet atelier."
      ],
      "metadata": {
        "id": "AcDMzwLhaFyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici la distribution de la colonne `Retweets received`."
      ],
      "metadata": {
        "id": "cNHccN-mc6Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Retweets received'].describe()"
      ],
      "metadata": {
        "id": "3tnUU_iVQZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En suivant les percentiles qui sont affichés dans la distribution de la colonne `Retweets received` (résultat du bloc de code précedent), ajoutez la valeur de découpage manquante dans la liste des `bins` dans le bloc de code qui suit. \n",
        "Divisez le nombre de Retweet en quatres catégories: \n",
        "1. `low`, qui regroupe les tweets ayant recu un faible suivi\n",
        "2. `medium`, qui regroupe les tweets ayant recu un suivi moyen\n",
        "3. `high`, qui regroupe les tweets ayant recu un grand suivi\n",
        "4. `very_high`, qui regroupe les tweets ayant recu un très grand suivi"
      ],
      "metadata": {
        "id": "RPuQijA6dAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, 161, ..., ..., 449711]\n",
        "names = ['low', 'medium', 'high', 'very_high']\n",
        "df['Retweets_received_category']  = pd.cut(df['Retweets received'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "Z25CYh43QRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choisir la **catégorie cible** pour laquelle analyser les spécificités lexicales. La valeur doit être une des quatres valeurs contenues dans la colonne `Retweets_received_category` générée dans le bloc de code précédent."
      ],
      "metadata": {
        "id": "NcugGIyneFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Retweets_received_category'] == ..."
      ],
      "metadata": {
        "id": "WT1B3Iq-RIPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executez l'analyse de spécificités."
      ],
      "metadata": {
        "id": "NQ9FdJfnf9qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "q6kVV9xjRM9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Étudier les différences lexicales entre les profils Tweeter </font>\n",
        "\n",
        "Dans le prochain exercice, sélectionnez deux ou trois profils Tweeter de votre choix et comparez les spécificités lexicales en étudiant plusieurs combinaisons de POS tag. \n",
        "\n",
        "Quelles sont les grandes différences lexicales entre les profils que vous avez choisis ? \n",
        "\n",
        "Voici la liste complète des profils présents dans le corpus et enregistrés sous la colonne `Profile Account` et les nombres de tweets par profil."
      ],
      "metadata": {
        "id": "uIiCPShTgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['Profile Account'])"
      ],
      "metadata": {
        "id": "Jfl6H0JV0v53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]\n",
        "\n",
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "yYTkjK6jgWa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Profile Account'] == ...\n",
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8-o-jsBI0pof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.3 NOTES PERSONNELLES: </font>\n",
        "\n",
        "-----\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "IhZSp6JKjH8k"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
