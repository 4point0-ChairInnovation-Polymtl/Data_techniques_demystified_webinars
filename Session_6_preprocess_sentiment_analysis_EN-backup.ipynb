{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNgJtQ3FrN8"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font size='10' color = 'E3A440'>**Megadata and Advanced Techniques Demystified**</font>\n",
        "=======\n",
        "<font color = 'E3A440'>*New Analysis Methods and their Implications for Megadata Management in SSH (part 1)*</font>\n",
        "=============\n",
        "\n",
        "\n",
        "This workshop is part of the training [Megadata and Advanced Techniques Demystified](https://www.4point0.ca/en/2022/08/22/formation-megadonnees-demystifiees//) (session 6).\n",
        "\n",
        "Humanities and social sciences are often confronted with the analysis of unstructured data, such as text. After preparing the data, several analysis techniques from machine learning can be used. During this workshop, participants will be introduced to the preprocessing of textual data and to supervised and unsupervised methods for analysis purposes with Python.\n",
        "\n",
        "Note: This workshop continues with a 2nd session on **November 10, 2022**. The two sessions cannot be considered exhaustive of the field.\n",
        "\n",
        "Structure of the workshop :\n",
        "1. Part 1 : Presentation of the [Section 1](#Section_1) in plenary mode (40 minutes)\n",
        "2. Part 2 : Individual work on [Section 2](#Section_2) (10 minutes)\n",
        "3. Part 3 : Team work on [Section 2](#Section_2) (30 minutes)\n",
        "4. Part 4 : Conclusion in plenary mode (10 minutes)\n",
        "\n",
        "### Autors: \n",
        "- Bruno Agard <bruno.agard@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "\n",
        "Département de Mathématiques et de génie industriel\n",
        "\n",
        "École Polytechnique de Montréal"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'E3A440'>0. Preparation environnement </font>"
      ],
      "metadata": {
        "id": "HpAX32-4Kj8U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jddhTL7EFrN_",
        "outputId": "30e167ea-98a3-4a9a-83f1-549f0dd2d33b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Data_techniques_demystified_webinars'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 30 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (30/30), done.\n"
          ]
        }
      ],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf Data_techniques_demystified_webinars/\n",
        "!git clone https://github.com/4point0-ChairInnovation-Polymtl/Data_techniques_demystified_webinars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJLRFWa6FrOA",
        "outputId": "2b680566-4696-462e-d809-6bec9afd9ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Import modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  <font color = 'E3A440'>**0.1 The basics of Python to carry out this workshop**</font>\n",
        "\n",
        "We present two basic concepts that will help people who have never written a line of code to orient themselves during this workshop."
      ],
      "metadata": {
        "id": "1_J-haWRfr9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'E3A440'>Variables</font>\n",
        "\n",
        "[Variables](https://www.w3schools.com/python/python_variables.asp) are objects that contain information in very different formats. To create a variable, we use the following syntax: `variable_name = content_of_the_variable`. Some example follows:"
      ],
      "metadata": {
        "id": "h9OcrUM3ghbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with characters\n",
        "variable_avec_caracteres = 'contenu de la variable avec caractères'"
      ],
      "metadata": {
        "id": "2NOP4itmggHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(variable_avec_caracteres)"
      ],
      "metadata": {
        "id": "ZY32KyFIl01S",
        "outputId": "964a5bb1-baf2-455d-c57d-a06729ab318f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contenu de la variable avec caractères\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with numbers\n",
        "variable_avec_nombre = 5\n",
        "print(variable_avec_nombre)"
      ],
      "metadata": {
        "id": "JYXwYERFjhIx",
        "outputId": "128d7d59-1179-4d80-d478-607af705e71c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with list of numbers\n",
        "variable_avec_liste_de_nombres = [5, 6, 7, 8, 9]\n",
        "print(variable_avec_liste_de_nombres)"
      ],
      "metadata": {
        "id": "Lqo9w4GtjmzJ",
        "outputId": "c1b2d6ff-d661-4cd0-b611-4ca20d22fde8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable with tablar data\n",
        "variable_avec_donnes_tabulaires = pd.DataFrame([[1,2,3],[4,5,6]], columns = ['Col_1','Col_2','Col_3'], index = ['Row_1','Row_2'])"
      ],
      "metadata": {
        "id": "AGOkQeezj6Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(variable_avec_donnes_tabulaires)"
      ],
      "metadata": {
        "id": "DlOlvpICmPdp",
        "outputId": "fd9bad5a-5550-41cf-dd06-fa6ea6724d93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Col_1  Col_2  Col_3\n",
            "Row_1      1      2      3\n",
            "Row_2      4      5      6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'E3A440'>Fonctions</font>\n",
        "\n",
        "[Fonctions](https://www.w3schools.com/python/python_functions.asp) allow you to perform calculations based on one or more variables, data, etc. Functions generally have the following syntax:  `resultat_de_la_function = fonction(argument_1 = contenu_argument_1, argument_2 = contenu_argument_2, etc.)`. Here some examples: "
      ],
      "metadata": {
        "id": "ovKqRMXtrfNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultat_somme = sum(variable_avec_liste_de_nombres)\n",
        "print(resultat_somme)"
      ],
      "metadata": {
        "id": "y08LpuNUrbVQ",
        "outputId": "da374058-6c15-4418-fe0a-b83ab4210d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame( data = [[1,2,3],[4,5,6]], columns = ['Col_1','Col_2','Col_3'], index = ['Row_1','Row_2'] )"
      ],
      "metadata": {
        "id": "qitg26AnuTmI",
        "outputId": "0d3b62b4-c290-4ca1-8e9c-fd61ce40fbd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Col_1  Col_2  Col_3\n",
              "Row_1      1      2      3\n",
              "Row_2      4      5      6"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a725e2b6-2852-4e50-9e0d-27777ccc8713\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Col_1</th>\n",
              "      <th>Col_2</th>\n",
              "      <th>Col_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Row_1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Row_2</th>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a725e2b6-2852-4e50-9e0d-27777ccc8713')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a725e2b6-2852-4e50-9e0d-27777ccc8713 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a725e2b6-2852-4e50-9e0d-27777ccc8713');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwFKNpPQFrN-"
      },
      "source": [
        "<a name='Section_1'></a>\n",
        "# <font color = 'E3A440'>1. *Preparation of textual data*</font>\n",
        "\n",
        "Text data analysis involves the transformation of text into a mathematical object that can be used by algorithms and statistical models. This step is important because it allows to **structure** unstructured data, such as text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDS10IXPFrOB"
      },
      "source": [
        "###  <font color = 'E3A440'>**1.1 Basic steps of data preparation**</font>\n",
        "\n",
        "Let's take the following sentence to illustrate the steps that will allow us to transform it into structured information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9AVaykcFrOB"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For now, `sentence` is just a string. You can count the number of characters that make up this variable."
      ],
      "metadata": {
        "id": "eqqNBHnM57ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFqEU4iFrOC",
        "outputId": "2a043f33-4598-4a4e-a9fe-372ca1029c7f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowing the number of characters in a text may not be enough to analyze its content :-).\n",
        "\n",
        "We will see in the following blocks of code different analysis tools."
      ],
      "metadata": {
        "id": "59CuGGAL6KSN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWQNjvKIFrOD"
      },
      "source": [
        "#### <font color = 'E3A440'>*a. Tokenisation*</font>\n",
        "\n",
        "First of all, it can be useful to cut the initial string of characters into elementary linguistic units endowed with meaning, generally called \"words\".\n",
        "\n",
        "In the `nltk` module, there is a function (`word_tokenize()`) which allows to perform this operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzF3fiUWFrOE",
        "outputId": "3192cc8e-7e78-4c0b-c030-b16f59e79ab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['At', 'eight', \"o'clock\", ',', 'on', 'Thursday', 'morning', ',', 'the', 'great', 'Arthur', 'did', \"n't\", 'feel', 'VERY', 'good', '.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# The function word_tokenize() takes a sentence as his main argument.\n",
        "words = nltk.word_tokenize(sentence)\n",
        "print(words)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Morphosyntactic analysis*</font>\n",
        "\n",
        "After having identified all the words, it is possible to analyze their morphosyntactic role, for analysis and/or filtering purposes."
      ],
      "metadata": {
        "id": "CvM6JKj1hLX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The function word_tokenize() takes a list of words as his main argument.\n",
        "words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ],
      "metadata": {
        "id": "qy06di1ygtmF",
        "outputId": "dd619c8c-ac06-4d71-cc2c-24821884fe42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), (',', '.'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), (',', '.'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the list of all possibles POS tags:"
      ],
      "metadata": {
        "id": "6cECT-Gp2obD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "kUDRUVYe2kLi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4f_8mvLFrOE"
      },
      "source": [
        "#### <font color = 'E3A440'>*c. Removing the punctuation*</font>\n",
        "\n",
        "One other operation consists in removing the punctuation. This type of filtering reduces those graphic signs having a lower contribution to the construction of the semantics of the sentence.\n",
        "In some contexts, such as in stylometry, this process can be performed with more sophisticated techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P7SkM5RFrOF",
        "outputId": "38f8475d-8963-4ef1-8db1-1f1663dec371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# The following line of code loop over each \"word\" ans keep those which contains only alphanumeric characters.\n",
        "words_pos1 = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "print(words_pos1)\n",
        "len(words_pos1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It is possible to use laso the resul of morphosyntactic analysis in order to remove punctuation.\n",
        "words_pos2 = [(w, pos) for w, pos in words_pos if pos != '.']\n",
        "print(words_pos2)\n",
        "len(words_pos2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S30VdMLP_6lC",
        "outputId": "40ebde81-7ad9-4eae-9044-21f447349385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('At', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('on', 'ADP'), ('Thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('Arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('VERY', 'ADV'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the difference: the \"words\" `o'clock` and `n't` are absent from the first list, but present in the second."
      ],
      "metadata": {
        "id": "QpCrw1VMAcbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_pos = words_pos2"
      ],
      "metadata": {
        "id": "hVEip9IyBOcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwcORV3vFrOG"
      },
      "source": [
        "#### <font color = 'E3A440'>*d. Convert each character to lowercase*</font>\n",
        "\n",
        "This step constitutes a first operation of standardization of the words and their reduction to a single graphic form. This kind of step makes it possible to group each occurrence of a word in a single form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8tdwmniFrOG",
        "outputId": "30f67c10-15b2-4258-8fcb-3a9fd50af145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('at', 'ADP'), ('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('on', 'ADP'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('the', 'DET'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('did', 'VERB'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('very', 'ADV'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# La ligne de code suivant itère sur chaque signe graphique et le transforme en minuscule.\n",
        "words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "print(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd1RxnCnFrOH"
      },
      "source": [
        "#### <font color = 'E3A440'>*e. Removing stopwords*</font>\n",
        "\n",
        "Another filtering operation consists in eliminating functional words, names **stopwords**. This word list contains all sentence connectors, such as \"and\", \"but\", \"however\" and words with low semantic value, such as modal verbs.\n",
        "Like other filtering operations, the challenge is to clean up the vocabulary as much as possible and to reduce all occurrences of a word to a single graphical form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icU72m20FrOH",
        "outputId": "96af1ae1-baac-4699-fa2d-29685fdcac04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ],
      "source": [
        "# Here , we import a stopwrod list for English\n",
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws2STtGKFrOH",
        "outputId": "e6ae784f-615a-4e35-9c26-3e946cfe4dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# The following line of code loop over each word of the sentence and keep those whici does not correspond to a stopword.\n",
        "words_pos = [(w, pos) for w, pos in words_pos if w not in stopwords.words(\"english\")]\n",
        "print(words_pos)\n",
        "len(words_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8acb4VWFrOI"
      },
      "source": [
        "#### <font color = 'E3A440'>*f. Stemming or lemmatization*</font> \n",
        "\n",
        "Following the same objective, we remove the morphological suffix from words, which increases the level of reduction of each occurrence of a word to a unique graphic form.\n",
        "\n",
        "There are two fundamental methods: stemming and lemmatization.\n",
        "The first reduces the occurrences to a stem which is inferred by means of several techniques, the other is the reduction of the occurrence to its lemma."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_mhuA7zFrOI",
        "outputId": "c5a46266-5312-4c8c-ebe6-f38764016387"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Stemming: Porter algorithm\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmed_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnsV3pSZFrOI",
        "outputId": "b1a31c39-c7d9-4cb7-a9d7-b0e86b3709be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('thursday', 'NOUN'), ('morn', 'NOUN'), ('gre', 'ADJ'), ('arth', 'NOUN'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Stemming: Lancaster algorithm\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmed_pos = [(LancasterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "print(stemmed_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7uAtUgFrOJ",
        "outputId": "ab176422-45ab-4437-d82d-2d4432d292a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('eight', 'NUM'), (\"o'clock\", 'NOUN'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), (\"n't\", 'ADV'), ('feel', 'VERB'), ('good', 'ADJ')]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization: using thesaurus of wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmed_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "print(lemmed_pos)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*g. Filtering by morphosyntactic role*</font>\n",
        "\n",
        "The filtering of lexical units can be extended to the elimination of units which does not belong to a list of predefined morphosyntactic roles. For example, we can remove all words that are not *nouns* or *adjectives*."
      ],
      "metadata": {
        "id": "n-48nGApkl_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep nouns and adjectives only.\n",
        "lemmed_pos = [(w, pos) for w, pos in words_pos if pos in ['NOUN','ADJ']]\n",
        "print(lemmed_pos)"
      ],
      "metadata": {
        "id": "FrOSEBCzk08r",
        "outputId": "053efcd1-5540-460e-de6f-de7b660d3fa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"o'clock\", 'NOUN'), ('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('good', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ges85oatFrOJ"
      },
      "source": [
        "## <font color = 'E3A440'>**1.2 Processing of a corpus**</font>\n",
        "\n",
        "The pre-processing of a corpus of texts may require the implementation of several steps. The first and most important is the division of the corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*1. Text splitting*</font>\n",
        "\n",
        "Depending on the purpose of the analysis, the text can be split into several fragments, that can be a document, a paragraph, a concordance, a group of sentences, a single sentence, etc.\n",
        "\n"
      ],
      "metadata": {
        "id": "GZqenN7Rcc2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOzWtcugFrOK",
        "outputId": "ed603621-8445-4765-afb6-1407ad967c1a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "text = \"\"\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\n",
        "          The following morning, at nine, Arthur felt better.\n",
        "          A dog run in the street.\"\"\"\n",
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next block of code, we do a sentence split."
      ],
      "metadata": {
        "id": "sy8Qow43dHKv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lihe1FNjFrOK",
        "outputId": "5a96b48d-e9f9-496d-9ead-a1b1a0520e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"At eight o'clock, on Thursday morning, the great Arthur didn't feel VERY good.\", 'The following morning, at nine, Arthur felt better.', 'A dog run in the street.']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*2. Annotation et cleaning*</font>\n",
        "\n",
        "The previous morphosyntactic annotation and filtering operations will be applied to each fragment of the corpus that has been created."
      ],
      "metadata": {
        "id": "gHi0hAxZdKx4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*a. Creating a function*</font>\n",
        "\n",
        "In the following code, a function is created to encompass all the operations needed for annotation and cleaning."
      ],
      "metadata": {
        "id": "waMXvFxhgH0u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rtngk2rFrOL"
      },
      "outputs": [],
      "source": [
        "# To run this function proprlely, you need to import modules needed\n",
        "def CleaningText(text_as_string, language = 'english', reduce = '', list_pos_to_keep = [], Stopwords_to_add = []):\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    words = nltk.word_tokenize(text_as_string)\n",
        "    words_pos = nltk.pos_tag(words, tagset='universal')\n",
        "    words_pos = [(w, pos) for w, pos in words_pos if w.isalnum()]\n",
        "    words_pos = [(w.lower(), pos) for w, pos in words_pos]\n",
        "    \n",
        "    if reduce == 'stem': \n",
        "        from nltk.stem.porter import PorterStemmer\n",
        "        reduced_words_pos = [(PorterStemmer().stem(w), pos) for w, pos in words_pos]\n",
        "        \n",
        "    elif reduce == 'lemma':\n",
        "        from nltk.stem.wordnet import WordNetLemmatizer\n",
        "        reduced_words_pos = [(WordNetLemmatizer().lemmatize(w), pos) for w, pos in words_pos]\n",
        "    else:\n",
        "        import warnings\n",
        "        reduced_words_pos = words_pos\n",
        "        warnings.warn(\"Warning : any reduction was made on words! Please, use \\\"reduce\\\" argument to chosse between 'stem' or  'lemma'\")\n",
        "    if list_pos_to_keep:\n",
        "        reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if pos in list_pos_to_keep]\n",
        "    else:\n",
        "        import warnings\n",
        "        warnings.warn(\"Warning : any POS filtering was made. Please, use \\\"list_pos_to_keep\\\" to create a list of POS tag to keep.\")\n",
        "    \n",
        "    list_stopwords = stopwords.words(language) + Stopwords_to_add\n",
        "    reduced_words_pos = [(w, pos) for w, pos in reduced_words_pos if w not in list_stopwords and len(w) > 1 ]\n",
        "    return reduced_words_pos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*b. Application of cleaning*</font>\n",
        "\n",
        "Now we can apply this function to each text fragment."
      ],
      "metadata": {
        "id": "kWkIfcMwpqe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['NOUN','ADJ','VERB']) for sent in sentences]\n",
        "print(cleaned_sentences)"
      ],
      "metadata": {
        "id": "vkpxyQfl2Wna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd69692-d386-43ae-d08e-75e48b1d59c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[('thursday', 'NOUN'), ('morning', 'NOUN'), ('great', 'ADJ'), ('arthur', 'NOUN'), ('feel', 'VERB'), ('good', 'ADJ')], [('following', 'ADJ'), ('morning', 'NOUN'), ('arthur', 'NOUN'), ('felt', 'VERB')], [('dog', 'NOUN'), ('run', 'NOUN'), ('street', 'NOUN')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color = 'E3A440'>*c. Frequency of words*</font>\n",
        "\n",
        "What is the frequency of the words in our corpus? To answer, we create a list of words by removing the morphosyntactic annotation."
      ],
      "metadata": {
        "id": "TBNxooILr8-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs_in_text = nltk.FreqDist([w for sent in cleaned_sentences for w, pos in sent ])\n",
        "freqs_in_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFPhg9Sm6Lbq",
        "outputId": "5735b5b2-60a5-41f0-ab85-e27ac44a255c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'morning': 2, 'arthur': 2, 'thursday': 1, 'great': 1, 'feel': 1, 'good': 1, 'following': 1, 'felt': 1, 'dog': 1, 'run': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'>*3. Vectorization*</font>\n",
        "\n",
        "Typically, to use text in a data analysis or machine learning context, text must be transformed into an appropriate mathematical object.\n",
        "The simplest and most widespread model is the \"bags-of-words\", in which each text (or each text fragment) is defined in a vector, by a certain number of lexical units which characterize it. This model belongs to the family of vector semantics models and it has the following form:\n",
        "\n",
        "\n",
        "$$X = \\begin{bmatrix} \n",
        "x_{1,1} & x_{1,2} & \\ldots & x_{1,w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n,1} & x_{1,2} & \\ldots & x_{n,w} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        "\n",
        "In this matrix, the value $x_{i,j}$ represents the \"weigth\" of the word $j$ in the text fragment $i$. This weigth can be computed in several way. Thus :\n",
        "\n",
        "- $x_{i,j}$ can represents the presence of the word \"j\" in text fragment $i$,\n",
        "- $x_{i,j}$ can measures the quantoty of occurrences of a word $j$ in text fragment $i$,\n",
        "- $x_{i,j}$ can represent the **value** of the word $j$ in text fragment $i$, and this, using metric such as tf-idf :\n",
        " $$\\text{tf-idf}_{i,j}=\\text{tf}_{i,j}.log\\left(\\frac{n}{n_i}\\right)$$\n",
        " - $\\text{tf}_{i,j}$ is the frequency of word $i$ in text fragment $j$,\n",
        " - $n$ total count of text fragments,\n",
        " - $n_i$ total counts of text fragments containing the word $i$.\n"
      ],
      "metadata": {
        "id": "m0YdoOA1stB2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Object initialization\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 1, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 10, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "LpFPJrGI2Nsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of the \"vectorizer\" with a list of word lists (and not a list of word-pos tuples)."
      ],
      "metadata": {
        "id": "G9uC3usB5dtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of list of words:\n",
        "[[w for w, pos in sent] for sent in cleaned_sentences]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy8_VTM67Lax",
        "outputId": "be0d883f-217b-4817-99ae-0c652e1e472d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['thursday', 'morning', 'great', 'arthur', 'feel', 'good'],\n",
              " ['following', 'morning', 'arthur', 'felt'],\n",
              " ['dog', 'run', 'street']]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Application of the vectorizer\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_sentences])\n",
        "print(pd.DataFrame(freq_term_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ydVdmC5c76",
        "outputId": "3557c02e-b273-419f-ad12-984cf4bcf776"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   arthur  dog  feel  felt  following  good  great  morning  run  street  \\\n",
            "0       1    0     1     0          0     1      1        1    0       0   \n",
            "1       1    0     0     1          1     0      0        1    0       0   \n",
            "2       0    1     0     0          0     0      0        0    1       1   \n",
            "\n",
            "   thursday  \n",
            "0         1  \n",
            "1         0  \n",
            "2         0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we assign the result of the Tf-IDF weighting to the variable named `tfidf_DTM`. "
      ],
      "metadata": {
        "id": "pRPjGQVokg7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tfidf matrix\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)\n",
        "print(pd.DataFrame(tfidf_DTM.todense(), columns =  [k for k, v in sorted(vectorized.vocabulary_.items(), key=lambda item: item[1])] ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GsktR7_Znke",
        "outputId": "3133a1e3-9eee-4cd2-bacb-250c08e9b7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     arthur       dog      feel      felt  following      good     great  \\\n",
            "0  0.137750  0.000000  0.181125  0.000000   0.000000  0.181125  0.181125   \n",
            "1  0.215994  0.000000  0.000000  0.284006   0.284006  0.000000  0.000000   \n",
            "2  0.000000  0.333333  0.000000  0.000000   0.000000  0.000000  0.000000   \n",
            "\n",
            "    morning       run    street  thursday  \n",
            "0  0.137750  0.000000  0.000000  0.181125  \n",
            "1  0.215994  0.000000  0.000000  0.000000  \n",
            "2  0.000000  0.333333  0.333333  0.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QymfV4S_FrOZ"
      },
      "source": [
        "<a name=\"Section_2\"></a>\n",
        "# <font color = 'E3A440'> 2. *Exercise : Sentiment Analysis on Twitter* </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The exercise proposed in this section is based on a simple processing chain for **sentiment analysis** on Twitter data and **analysis of lexical specificities**.\n",
        "\n",
        "The corpus used was collected in 2020 by *trackmyhashtag.com* and contains 150,000 tweets for the 50 most followed profiles on Twitter. The data is in tabular format in a CSV file. For pedagogical reasons, this exercise foresees the use of a random sample of 5,000 tweets.\n",
        "\n",
        "First, the textual data of 5,000 tweets will be analyzed by a sentiment analysis module of the `nltk` module. Then the text will be preprocessed and some lexical analysis will be performed.\n",
        "\n",
        "During the exercise, the participant will be invited to fill the missing parts of the code which are indicated with `...` (three dots)."
      ],
      "metadata": {
        "id": "c8XIxHR-n0Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.1 Presentation of the exercise </font>"
      ],
      "metadata": {
        "id": "FAgZaK170IHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Import data </font>\n",
        "\n",
        "The file with the data is archived in a `.zip` and contains more than 150,000 tweets. For educational reasons, we only import 5,000 random tweets. "
      ],
      "metadata": {
        "id": "IxtJNukjCXT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='Donnees_demystifiees_seance_6/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')\n",
        "import zipfile\n",
        "from datetime import datetime\n",
        "\n",
        "#Unzips the dataset and gets the TSV dataset\n",
        "with zipfile.ZipFile(os.path.join(DATA_DIR,'4POINT0_Top_50_tweet_profiles.zip'), 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)\n",
        "\n",
        "df = pd.read_pickle(os.path.join(DATA_DIR,'Top_50_tweet_profiles.pkl')).sample(5000, random_state = 5641).reset_index()"
      ],
      "metadata": {
        "id": "Q6snBPy8EfBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here available variables in the dataset."
      ],
      "metadata": {
        "id": "hDTMmSBhFA6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejgQAfv8FIrC",
        "outputId": "0483f974-c048-4989-ea17-5b465e9c02fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['index', 'Tweet Id', 'Tweet URL', 'Tweet Posted Time', 'Tweet Content',\n",
              "       'Tweet Type', 'Client', 'Retweets received', 'Likes received',\n",
              "       'User Id', 'Name', 'Username', 'Verified or Non-Verified',\n",
              "       'Profile URL', 'Protected or Not Protected', 'Profile Account'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here an observation (one row of the table of data):"
      ],
      "metadata": {
        "id": "JIDKgUxtGhQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "id": "54zPNygEJXhe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7434202-82a6-470a-b6a4-927be1bd8908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                                                     71560\n",
              "Tweet Id                                                     656538552327630848\n",
              "Tweet URL                     https://twitter.com/billboard/status/656538552...\n",
              "Tweet Posted Time                                           2015-10-20 18:32:43\n",
              "Tweet Content                 .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "Tweet Type                                                              Retweet\n",
              "Client                                                       Twitter Web Client\n",
              "Retweets received                                                         20260\n",
              "Likes received                                                            22109\n",
              "User Id                                                                 9695312\n",
              "Name                                                                  billboard\n",
              "Username                                                              billboard\n",
              "Verified or Non-Verified                                               Verified\n",
              "Profile URL                                       https://twitter.com/billboard\n",
              "Protected or Not Protected                                        Not Protected\n",
              "Profile Account                                                    justinbieber\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Run Sentiment Analysis </font>\n",
        "\n",
        "The `SentimentIntensityAnalyzer` object is used to perform sentiment analysis. The object must be initialized and then the `polarity_scores()` function can be applied to a string."
      ],
      "metadata": {
        "id": "ym3Y6ITXGkkj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pInoj9yrFrOZ",
        "outputId": "ee9e47ad-e63c-47d9-8230-61b9cd8035d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are three examples of sentiment analysis. The result of the `polarity_scores()` function returns four values:\n",
        "\n",
        " 1. `neg` : indicates the degree, on a scale from 0 to 1, of negative sentiment of the text.\n",
        " 2. `neu` : indicates the degree, on a scale of 0 to 1, of neutral sentiment of the text.\n",
        " 3. `pos` :indicates the degree, on a scale of 0 to 1, of positive sentiment of the text.\n",
        " 4. `compound` : contains a composite value of the previous three metrics with a range from -1 to 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "sggH1sSIIVib"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jAgx5YgFrOa",
        "outputId": "83c53e7f-5573-4054-8e38-013175f3ca9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.52, 'pos': 0.48, 'compound': 0.8516}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "sia.polarity_scores(\"Wow, Montreal Canadiens is the greatest hockey team in the world!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia.polarity_scores(\"Ottawa is not bad city!\")"
      ],
      "metadata": {
        "id": "ifi6teSvITFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa3d600-b55d-419e-94c3-af0561e2d708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.56, 'pos': 0.44, 'compound': 0.484}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpGJWGRWFrOa",
        "outputId": "be7ed603-be20-40c1-edaf-2aadfbbfb975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.436, 'neu': 0.564, 'pos': 0.0, 'compound': -0.7339}"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "sia.polarity_scores(\"No, you cannot put pineapple on a pizza! This is disgusting!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are tweets on which we will apply the sentiment analysis:"
      ],
      "metadata": {
        "id": "3gfG4_A6I4WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tweet Content']"
      ],
      "metadata": {
        "id": "5yBg-jU9QLl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e1d4e4c-6f05-4698-bf56-8ee61d6ac1a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "1       👏 ¡@SergioRamos y @hazardeden10 se encuentran ...\n",
              "2       Here's how the market may predict the next pre...\n",
              "3       “Children are magical on road trips. They have...\n",
              "4       .@MelissaMcCarthy told me about the moment she...\n",
              "                              ...                        \n",
              "4995    So saddened to hear of the tragic theatre shoo...\n",
              "4996    always takes the road less traveled... @ New O...\n",
              "4997    #HustleHart #MoveWithHart https://t.co/GkQHkhKhR3\n",
              "4998    This is the letter the US Attorney General sen...\n",
              "4999    The Week on Instagram | 276\\nhttps://t.co/9kIt...\n",
              "Name: Tweet Content, Length: 5000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next block of code, we run sentiment analysis on the `Tweet Content` column, and add the resulting results to the data table (the object named `df`)."
      ],
      "metadata": {
        "id": "aCfTrGKYNOcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Running Sentiment Analysis on the Corpus\n",
        "datasent = df.apply(lambda x: sia.polarity_scores(x['Tweet Content']), 1)\n",
        "df = df.join(pd.DataFrame(list(datasent)))"
      ],
      "metadata": {
        "id": "3etQFxEACpty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result of the analysis is saved in 4 variables. Here is an example:"
      ],
      "metadata": {
        "id": "bpauZXt7Nwyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfrp02peNwSu",
        "outputId": "38ccb729-93db-4158-a681-48c8b6e4ba1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "index                                                                     71560\n",
              "Tweet Id                                                     656538552327630848\n",
              "Tweet URL                     https://twitter.com/billboard/status/656538552...\n",
              "Tweet Posted Time                                           2015-10-20 18:32:43\n",
              "Tweet Content                 .@JustinBieber, @Skrillex and @Bloodpop's #Sor...\n",
              "Tweet Type                                                              Retweet\n",
              "Client                                                       Twitter Web Client\n",
              "Retweets received                                                         20260\n",
              "Likes received                                                            22109\n",
              "User Id                                                                 9695312\n",
              "Name                                                                  billboard\n",
              "Username                                                              billboard\n",
              "Verified or Non-Verified                                               Verified\n",
              "Profile URL                                       https://twitter.com/billboard\n",
              "Protected or Not Protected                                        Not Protected\n",
              "Profile Account                                                    justinbieber\n",
              "neg                                                                        0.12\n",
              "neu                                                                        0.88\n",
              "pos                                                                         0.0\n",
              "compound                                                                 -0.128\n",
              "Name: 0, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the analysis simple, we will only use the compound metric `compound` which is automatically calculated by the `polarity_score()` function."
      ],
      "metadata": {
        "id": "wsjs2wOmbs0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['compound'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf_vn9Veebrg",
        "outputId": "1bbb36a4-194e-4bbb-f9cb-83d8baf31524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    5000.000000\n",
              "mean        0.199087\n",
              "std         0.417216\n",
              "min        -0.972600\n",
              "25%         0.000000\n",
              "50%         0.000000\n",
              "75%         0.557400\n",
              "max         0.980200\n",
              "Name: compound, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the `compound` metric in a **lexical specificity analysis** context, it is necessary to constitute categories, i.e. to group the tweets under the following categories:\n",
        " 1. `negative` : which groups tweets containing negative sentiment (`compound` from -1 to -0.1) \n",
        " 2. `neu` : which groups tweets that are more neutral (`compound` from -0.5 to 0.5)\n",
        " 3. `positive` : which groups tweets containing a positive sentiment (`compound` more than 0.5)"
      ],
      "metadata": {
        "id": "A6cLssRqb81P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Determine the values ​​to cut the compound metric\n",
        "bins = [-1, -0.1, 0.5, 1]\n",
        "# 2 Determine the names of the categories. NOTE that the numbers of category names must be less than the cut values.\n",
        "names = ['negative', 'neu', 'positive']\n",
        "# Execute slicing with pandas 'cut' function.\n",
        "df['compound_category']  = pd.cut(df['compound'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "jJrj4DKePhGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the distribution of tweets by category:"
      ],
      "metadata": {
        "id": "-cRjoTWDdJjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['compound_category'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL-ibgrb4u0M",
        "outputId": "c8a02b8d-7036-4c8a-db1a-85df37b47736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'negative': 630, 'neu': 2969, 'positive': 1401})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Annotation, cleaning and vectorization of tweets </font>\n",
        "\n",
        "We use the previously written function to clean the lexical units of tweets. For this first test, we keep only the adjectives.\n",
        "\n",
        "This operation will take a few seconds."
      ],
      "metadata": {
        "id": "_zmlEGCKRaWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = ['ADJ'], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "NMmjI8IaU_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the vectorization step we retain the words that appear in at least 5 documents (`min_df = 5`)."
      ],
      "metadata": {
        "id": "TA9YRrWiLzw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Object initialization\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "6Aj4JUdeYBbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkSkuQvUXsvs",
        "outputId": "8c8c802d-5135-4842-f8d9-15e9c42e6c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'b', 'c', 'e', 'f', 'g', 'h', 'j', 'l', 'n', 'p', 'r', 'u', 'v', 'w'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5000x172 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 2661 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> d. Analysis of lexical specificities </font>\n",
        "\n",
        "The analysis of lexical specificities makes it possible to highlight the lexical units which are specific to a particular group of data. In our case, it is possible to identify the words that are more strongly associated with positive or negative feelings.\n",
        "\n",
        "To do this, we use a widely used metric in lexicometry, which is the likelihood function (log-likelihood ratio). The metric is based on this [article](https://aclanthology.org/J93-1003.pdf). Other methods can be used, such as mutual information, chi2 or tf-idf weighting."
      ],
      "metadata": {
        "id": "RtSfYp1rfUxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def GetLexicalSpecificities(freq_term_DTM, logical_vector):\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "    import math\n",
        "    df_freq_target = pd.DataFrame(np.asarray(freq_term_DTM[logical_vector].sum(0).T).reshape(-1))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(freq_term_DTM[~(logical_vector)].sum(0).T).reshape(-1)\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    frequency_threshold = 10 # Insert your frequency threshold as integer\n",
        "    return df_freq_target[df_freq_target['tot'] > frequency_threshold]['Log-likelihood Ratio'].sort_values(ascending=False).iloc[range(50)]"
      ],
      "metadata": {
        "id": "39eDHLjFZcLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform the specificity analysis, it is necessary to create a logical vector (with binary values) which indicates with `True` the class for which we want to analyze the lexical specificity and with `False` the rest of the corpus."
      ],
      "metadata": {
        "id": "SIxwMCSahwh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == 'positive'\n",
        "logical_vector"
      ],
      "metadata": {
        "id": "TVhrPm7PZO5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3edf5076-5454-4c1a-9383-87d21de63a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       False\n",
              "1       False\n",
              "2       False\n",
              "3       False\n",
              "4       False\n",
              "        ...  \n",
              "4995    False\n",
              "4996    False\n",
              "4997    False\n",
              "4998    False\n",
              "4999    False\n",
              "Name: compound_category, Length: 5000, dtype: bool"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(logical_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaDNEUNZQU0",
        "outputId": "0399f211-9c0b-432e-a06d-47dff32d0a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "630"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the function with the frequency matrix (`freq_term_DTM`) and the logical vector we created above."
      ],
      "metadata": {
        "id": "G3lrLVXfiMMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8iEQNaJvJ2fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728cc8e3-9b7c-4608-fd2c-a43ab295333b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "free          27.839934\n",
              "beautiful     27.772819\n",
              "amazing        5.374933\n",
              "happy          4.477503\n",
              "great          3.618858\n",
              "perfect        3.448933\n",
              "best           3.086363\n",
              "proud          2.586437\n",
              "special        2.496239\n",
              "much           1.311430\n",
              "good           1.246304\n",
              "whole          1.127005\n",
              "incredible     0.957080\n",
              "important      0.934360\n",
              "nice           0.863971\n",
              "excited        0.827445\n",
              "better         0.711968\n",
              "powerful       0.711968\n",
              "favorite       0.629506\n",
              "single         0.319650\n",
              "huge           0.296930\n",
              "hard           0.296930\n",
              "funny          0.127005\n",
              "big            0.127005\n",
              "exclusive     -0.024998\n",
              "young         -0.065640\n",
              "able          -0.136029\n",
              "sure          -0.153103\n",
              "many          -0.168451\n",
              "ready         -0.194923\n",
              "top           -0.220918\n",
              "old           -0.235565\n",
              "high          -0.288032\n",
              "last          -0.407331\n",
              "long          -0.525071\n",
              "available     -0.661491\n",
              "american      -0.661491\n",
              "black         -0.680350\n",
              "new           -0.757518\n",
              "next          -0.811594\n",
              "true          -0.872995\n",
              "le            -0.872995\n",
              "little        -0.872995\n",
              "entire        -0.988472\n",
              "social        -1.042920\n",
              "real          -1.095387\n",
              "live          -1.162501\n",
              "official      -1.288032\n",
              "final         -1.288032\n",
              "military      -1.288032\n",
              "Name: Log-likelihood Ratio, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del logical_vector, freq_term_DTM"
      ],
      "metadata": {
        "id": "F9vvmgX46O7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.2 Exercice </font>\n",
        "\n",
        "During the exercise, the participant are invited to fill the missing parts of the code which are indicated with `...` (three dots).\n",
        "\n",
        "Several manipulations and different results will be required. Each sub-exercise follows this processing chain:\n",
        "\n",
        "1. Annotation and cleaning of tweets: the participant will have to adjust some parameters of the function to choose a specific filtering.\n",
        "2. Vectorization: the participant will have to adjust some parameters of the function to choose a specific filtering.\n",
        "3. Creation of a logical vector to define the target group and the reference group.\n",
        "4. Application of the `GetLexicalSpecificities()` function to obtain the 50 most specific words for the target group.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8zRK37miNzH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> a. Study the impact of morphosyntactic filtering on lexical specificities </font>\n",
        "\n",
        "In point 2.1, only adjectives have been studied. Now do a study on nouns, adjectives and verbs and then on other combinations that are interesting for you.\n",
        "Here is the list of existing POS tags:"
      ],
      "metadata": {
        "id": "Niv5IUK8VJ8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **POS** | **DESCRIPTION**           | **EXAMPLES**                                      |\n",
        "| ------- | ------------------------- | ------------------------------------------------- |\n",
        "| ADJ     | adjective                 | big, old, green, incomprehensible, first      |\n",
        "| ADP     | adposition                | in, to, during                                |\n",
        "| ADV     | adverb                    | very, tomorrow, down, where, there            |\n",
        "| AUX     | auxiliary                 | is, has (done), will (do), should (do)        |\n",
        "| CONJ    | conjunction               | and, or, but                                  |\n",
        "| CCONJ   | coordinating conjunction  | and, or, but                                  |\n",
        "| DET     | determiner                | a, an, the                                    |\n",
        "| INTJ    | interjection              | psst, ouch, bravo, hello                      |\n",
        "| NOUN    | noun                      | girl, cat, tree, air, beauty                  |\n",
        "| NUM     | numeral                   | 1, 2017, one, seventy-seven, IV, MMXIV        |\n",
        "| PART    | particle                  | ’s, not                                      |\n",
        "| PRON    | pronoun                   | I, you, he, she, myself, themselves, somebody |\n",
        "| PROPN   | proper noun               | Mary, John, London, NATO, HBO                 |\n",
        "| PUNCT   | punctuation               | ., (, ), ?                                    |\n",
        "| SCONJ   | subordinating conjunction | if, while, that                               |\n",
        "| SYM     | symbol                    | $, %, §, ©, +, −, ×, ÷, =, :)               |\n",
        "| VERB    | verb                      | run, runs, running, eat, ate, eating          |\n",
        "| X       | other                     | sfpksdpsxmsa                                  |\n",
        "| SPACE   | space                     |                                                   |\n"
      ],
      "metadata": {
        "id": "CJps3VlyWcoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert the correct value for the `list_pos_to_keep` argument in roder to keep nouns, adjectives and verbs, or any other POS tag combination of your interest.\n",
        "Look at the three dots `...` and fill it! You should inspire to prvious code presentend during this workshop. Copy-Paste is permitted!"
      ],
      "metadata": {
        "id": "b4bLCBgqWlzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]"
      ],
      "metadata": {
        "id": "xtrWcUOmNxOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the `min_df` parameters so that you don't exceed **750 words** of your matrix <font color='E3A440'>**Document-Term matrix**</font>, which is saved in the object `freq_term_DTM`.\n",
        "\n",
        "Note that in this function the `ngram_range` parameter is configured to have unigrams and bigrams (its value is: `(1,2)`)."
      ],
      "metadata": {
        "id": "g6JapeP7W2xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "    \n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 2), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "EoTIyCR7daI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the work already done in point **b.** of section **2.1**, choose the categories for which you want to study the lexical specificity ex. `negative` or `positive`."
      ],
      "metadata": {
        "id": "PC98Vv87YCL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['compound_category'] == ..."
      ],
      "metadata": {
        "id": "B-qMBlPhPGC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the function defined in point **d.** of section **2.1**, add the fundamental arguments of the function, i.e. the matrix <font color='E3A440'>**Document-Term matrix**</font> and the **logical vector** created in the previous code block."
      ],
      "metadata": {
        "id": "kUkik8fxYXe9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function needs 2 arguemnts: 1st is the matrix, the 2nd is the logical vector\n",
        "GetLexicalSpecificities(..., ...)"
      ],
      "metadata": {
        "id": "CGFUrGPVPJWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> b. Investigate new categories based on the number of Retweets </font>\n",
        "\n",
        "On Twitter, it is possible to retweet an existing tweet. The number of retweets can be considered an indicator of the interest a tweet has obtained.\n",
        "\n",
        "Answer the following question: what are the lexical specificities of tweets that have had a very large following?\n",
        "\n",
        "To answer, you must manipulate some line of code by performing the steps learned throughout this workshop."
      ],
      "metadata": {
        "id": "AcDMzwLhaFyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the distribution of the `Retweets received` column."
      ],
      "metadata": {
        "id": "cNHccN-mc6Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Retweets received'].describe()"
      ],
      "metadata": {
        "id": "3tnUU_iVQZBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following the percentiles that are displayed in the distribution of the `Retweets received` column (result of the previous chunk of code), add the missing slicing values to the `bins` list in the next chunk of code.\n",
        "Divide the number of Retweets into four categories:\n",
        "1. `low`, grouping tweets that have received a low interest\n",
        "2. `medium`, grouping tweets that have received a medium interest\n",
        "3. `high`, grouping tweets that have received a high interest\n",
        "4. `very_high`, grouping tweets that have received a very high interest"
      ],
      "metadata": {
        "id": "RPuQijA6dAR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bins = [-np.inf, 161, ..., ..., 449711]\n",
        "names = ['low', 'medium', 'high', 'very_high']\n",
        "df['Retweets_received_category']  = pd.cut(df['Retweets received'], bins, labels=names, include_lowest =True)"
      ],
      "metadata": {
        "id": "Z25CYh43QRBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the **target category** for which you want analyze the lexical specificities. The value must be one of the four values ​​contained in the `Retweets_received_category` column generated in the previous chunk of code."
      ],
      "metadata": {
        "id": "NcugGIyneFIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Retweets_received_category'] == ..."
      ],
      "metadata": {
        "id": "WT1B3Iq-RIPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the specificity analysis."
      ],
      "metadata": {
        "id": "NQ9FdJfnf9qW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "q6kVV9xjRM9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color = 'E3A440'> c. Study the lexical specificities ​​of different Tweet profiles </font>\n",
        "\n",
        "In the next exercise, select two or three Twitter profiles of your choice and compare the lexical specificities by studying several POS tag combinations. \n",
        "\n",
        "What are the main lexical differences between the profiles you have chosen?\n",
        "\n",
        "Here is the complete list of profiles present in the corpus and recorded under the `Profile Account` column and the number of tweets per profile."
      ],
      "metadata": {
        "id": "uIiCPShTgE8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(df['Profile Account'])"
      ],
      "metadata": {
        "id": "Jfl6H0JV0v53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Annotation and cleaning : ADD adjective and verbs as POS tag to keep\n",
        "cleaned_tweets = [CleaningText(sent, reduce = 'lemma', list_pos_to_keep = [...], Stopwords_to_add=['http']) for sent in list(df['Tweet Content'])]\n",
        "\n",
        "# 2. Vectorisation\n",
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "## 2.1 initialise with parameters : \n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 10, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 4500, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords.words('english'), # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "\n",
        "#\n",
        "freq_term_DTM = vectorized.fit_transform([[w for w, pos in sent] for sent in cleaned_tweets])\n",
        "\n",
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "yYTkjK6jgWa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logical_vector = df['Profile Account'] == ...\n",
        "GetLexicalSpecificities(freq_term_DTM, logical_vector)"
      ],
      "metadata": {
        "id": "8-o-jsBI0pof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'E3A440'> 2.3 NOTES PERSONNELLES: </font>\n",
        "\n",
        "-----\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "IhZSp6JKjH8k"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
